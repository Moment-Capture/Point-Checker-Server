{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNp/0oPpAcXdAOuXCMLtgMO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **drive mount**"],"metadata":{"id":"gvBpvu1woJw2"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lNETqEZ5n33v","executionInfo":{"status":"ok","timestamp":1700039676560,"user_tz":-540,"elapsed":24985,"user":{"displayName":"michelle","userId":"15485239787053820654"}},"outputId":"e7fc4a44-1fd5-478a-a3d6-826c5e445a24"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/MomentCapture/Colab"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c1uldqusn9ej","executionInfo":{"status":"ok","timestamp":1700037580076,"user_tz":-540,"elapsed":19,"user":{"displayName":"michelle","userId":"15485239787053820654"}},"outputId":"ad39d700-fb1c-4bf7-f52f-40fa54af6c8d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/MomentCapture/Colab\n"]}]},{"cell_type":"markdown","source":["# **requirements**"],"metadata":{"id":"kP87fKVwIold"}},{"cell_type":"markdown","source":["https://davelogs.tistory.com/77 참고"],"metadata":{"id":"3ZtgypUpoW-0"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fxb9qvANnwp5","executionInfo":{"status":"ok","timestamp":1700029111410,"user_tz":-540,"elapsed":22399,"user":{"displayName":"michelle","userId":"15485239787053820654"}},"outputId":"9ad8e1b7-d796-4d3a-bd29-32d08b637668"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'TextRecognitionDataGenerator'...\n","remote: Enumerating objects: 1565, done.\u001b[K\n","remote: Total 1565 (delta 0), reused 0 (delta 0), pack-reused 1565\u001b[K\n","Receiving objects: 100% (1565/1565), 152.62 MiB | 14.33 MiB/s, done.\n","Resolving deltas: 100% (651/651), done.\n","Updating files: 100% (576/576), done.\n","\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements-hw.txt'\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["# 소스코드 내려받기\n","!git clone https://github.com/Belval/TextRecognitionDataGenerator.git\n","\n","# 개발환경 구축\n","!pip3 install -r requirements.txt\n","\n","# 개발환경 구축 (수기 관련 문자열 지원 필요시)\n","!pip3 install -r requirements-hw.txt"]},{"cell_type":"code","source":["!pip install -r requirements.txt"],"metadata":{"id":"ZaocrNFfoSRZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9v2OZHyXPLCU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **학습데이터 csv**"],"metadata":{"id":"86Q-l7xqvBPo"}},{"cell_type":"code","source":["path = \"/content/drive/MyDrive/MomentCapture/Colab/EasyOCR/trainer/all_data\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ky4EMtiJvoFN","executionInfo":{"status":"ok","timestamp":1700031859404,"user_tz":-540,"elapsed":10,"user":{"displayName":"michelle","userId":"15485239787053820654"}},"outputId":"33ff70e4-2e03-435c-e0e1-573983ce7dde"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/MomentCapture/Colab/EasyOCR\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import os\n","\n","file_path = \"/content/drive/MyDrive/MomentCapture/Colab/EasyOCR/trainer/all_data/en_train_filtered\"\n","file_names = os.listdir(file_path)\n","\n","i = 1\n","for name in file_names:\n","    src = os.path.join(file_path, name)\n","    dst = '2021_' + str(i) + '.jpg'\n","    dst = os.path.join(file_path, dst)\n","    os.rename(src, dst)\n","    i += 1\n","\n","df = pd.DataFrame(columns=['filename', 'words'])\n","\n","dirlist = os.listdir(file_path)\n","\n","for file in dirlist:\n","    new_row = {'filename': file}\n","    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n","\n","display(df)\n","\n","df.to_csv('/content/drive/MyDrive/MomentCapture/Colab/EasyOCR/trainer/all_data/en_train_filtered/labels.csv', index=False, encoding=\"utf-8\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"6IbxRXqqvETU","executionInfo":{"status":"ok","timestamp":1700038046537,"user_tz":-540,"elapsed":460,"user":{"displayName":"michelle","userId":"15485239787053820654"}},"outputId":"4ccd610a-ee02-4dbd-c60c-0a580729e0af"},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":["         filename words\n","0      2021_1.jpg   NaN\n","1      2021_2.jpg   NaN\n","2      2021_3.jpg   NaN\n","3      2021_4.jpg   NaN\n","4      2021_5.jpg   NaN\n","..            ...   ...\n","179  2021_180.jpg   NaN\n","180  2021_181.jpg   NaN\n","181  2021_182.jpg   NaN\n","182  2021_183.jpg   NaN\n","183  2021_184.jpg   NaN\n","\n","[184 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-a2eb7d45-b2b5-4c9d-8d6f-756d3e4e170f\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>filename</th>\n","      <th>words</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2021_1.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2021_2.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2021_3.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2021_4.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2021_5.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>179</th>\n","      <td>2021_180.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>180</th>\n","      <td>2021_181.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>181</th>\n","      <td>2021_182.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>182</th>\n","      <td>2021_183.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>183</th>\n","      <td>2021_184.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>184 rows × 2 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a2eb7d45-b2b5-4c9d-8d6f-756d3e4e170f')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-a2eb7d45-b2b5-4c9d-8d6f-756d3e4e170f button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-a2eb7d45-b2b5-4c9d-8d6f-756d3e4e170f');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-3b796c00-cf47-4512-8ec2-2a45a46b84fc\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3b796c00-cf47-4512-8ec2-2a45a46b84fc')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-3b796c00-cf47-4512-8ec2-2a45a46b84fc button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{}}]},{"cell_type":"code","source":["import pandas as pd\n","import os\n","\n","file_path = \"/content/drive/MyDrive/MomentCapture/Colab/EasyOCR/trainer/all_data/en_val\"\n","file_names = os.listdir(file_path)\n","\n","i = 1\n","for name in file_names:\n","    src = os.path.join(file_path, name)\n","    dst = '2021_' + str(i) + '.jpg'\n","    dst = os.path.join(file_path, dst)\n","    os.rename(src, dst)\n","    i += 1\n","\n","df = pd.DataFrame(columns=['filename', 'words'])\n","\n","dirlist = os.listdir(file_path)\n","\n","for file in dirlist:\n","    new_row = {'filename': file}\n","    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n","\n","display(df)\n","\n","df.to_csv('/content/drive/MyDrive/MomentCapture/Colab/EasyOCR/trainer/all_data/en_val/labels.csv', index=False, encoding=\"utf-8\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"uYPhfnJ1KVQh","executionInfo":{"status":"ok","timestamp":1700038137291,"user_tz":-540,"elapsed":468,"user":{"displayName":"michelle","userId":"15485239787053820654"}},"outputId":"751d0fc1-524b-4a89-eb96-1984d0171dfd"},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":["       filename words\n","0    2021_1.jpg   NaN\n","1    2021_2.jpg   NaN\n","2    2021_3.jpg   NaN\n","3    2021_4.jpg   NaN\n","4    2021_5.jpg   NaN\n","5    2021_6.jpg   NaN\n","6    2021_7.jpg   NaN\n","7    2021_8.jpg   NaN\n","8    2021_9.jpg   NaN\n","9   2021_10.jpg   NaN\n","10  2021_11.jpg   NaN\n","11  2021_12.jpg   NaN\n","12  2021_13.jpg   NaN\n","13  2021_14.jpg   NaN\n","14  2021_15.jpg   NaN\n","15  2021_16.jpg   NaN\n","16  2021_17.jpg   NaN\n","17  2021_18.jpg   NaN\n","18  2021_19.jpg   NaN\n","19  2021_20.jpg   NaN\n","20  2021_21.jpg   NaN\n","21  2021_22.jpg   NaN\n","22  2021_23.jpg   NaN\n","23  2021_24.jpg   NaN\n","24  2021_25.jpg   NaN\n","25  2021_26.jpg   NaN\n","26  2021_27.jpg   NaN\n","27  2021_28.jpg   NaN\n","28  2021_29.jpg   NaN\n","29  2021_30.jpg   NaN\n","30  2021_31.jpg   NaN\n","31  2021_32.jpg   NaN\n","32  2021_33.jpg   NaN\n","33  2021_34.jpg   NaN\n","34  2021_35.jpg   NaN\n","35  2021_36.jpg   NaN\n","36  2021_37.jpg   NaN\n","37  2021_38.jpg   NaN\n","38  2021_39.jpg   NaN\n","39  2021_40.jpg   NaN\n","40  2021_41.jpg   NaN\n","41  2021_42.jpg   NaN\n","42  2021_43.jpg   NaN\n","43  2021_44.jpg   NaN\n","44  2021_45.jpg   NaN\n","45  2021_46.jpg   NaN\n","46  2021_47.jpg   NaN\n","47  2021_48.jpg   NaN\n","48  2021_49.jpg   NaN\n","49  2021_50.jpg   NaN\n","50  2021_51.jpg   NaN"],"text/html":["\n","  <div id=\"df-ca73839e-39f2-4b0f-bee6-49b56b835a28\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>filename</th>\n","      <th>words</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2021_1.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2021_2.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2021_3.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2021_4.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2021_5.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>2021_6.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>2021_7.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>2021_8.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>2021_9.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>2021_10.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>2021_11.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>2021_12.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>2021_13.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>2021_14.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>2021_15.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>2021_16.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>2021_17.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>2021_18.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>2021_19.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>2021_20.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>2021_21.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>2021_22.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>2021_23.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>2021_24.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>2021_25.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>2021_26.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>2021_27.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>2021_28.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>2021_29.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>2021_30.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>2021_31.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>2021_32.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>2021_33.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>2021_34.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>2021_35.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>2021_36.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>2021_37.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>2021_38.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>2021_39.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>2021_40.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>2021_41.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>2021_42.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>2021_43.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>2021_44.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>2021_45.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>2021_46.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>2021_47.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>2021_48.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>2021_49.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>2021_50.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>50</th>\n","      <td>2021_51.jpg</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ca73839e-39f2-4b0f-bee6-49b56b835a28')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-ca73839e-39f2-4b0f-bee6-49b56b835a28 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-ca73839e-39f2-4b0f-bee6-49b56b835a28');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-c39c0ea3-c19b-4d6d-9e1b-91b6d7e1c77b\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c39c0ea3-c19b-4d6d-9e1b-91b6d7e1c77b')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-c39c0ea3-c19b-4d6d-9e1b-91b6d7e1c77b button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{}}]},{"cell_type":"code","source":["df.to_csv('/content/drive/MyDrive/MomentCapture/Colab/EasyOCR/trainer/all_data/gt.csv', index=False, encoding=\"utf-8\")"],"metadata":{"id":"wu7154scvFYR","executionInfo":{"status":"ok","timestamp":1700032017319,"user_tz":-540,"elapsed":348,"user":{"displayName":"michelle","userId":"15485239787053820654"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["# **train**"],"metadata":{"id":"YioPadkJ3L7c"}},{"cell_type":"code","source":["%cd /content/drive/MyDrive/MomentCapture/Colab/EasyOCR\n","\n","!pip install easyocr"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NvCGWb1o3Not","executionInfo":{"status":"ok","timestamp":1700039684924,"user_tz":-540,"elapsed":8367,"user":{"displayName":"michelle","userId":"15485239787053820654"}},"outputId":"5ea5aa04-3bf6-469e-f87c-6b90e8270339"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/MomentCapture/Colab/EasyOCR\n","Collecting easyocr\n","  Downloading easyocr-1.7.1-py3-none-any.whl (2.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.1.0+cu118)\n","Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.16.0+cu118)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from easyocr) (4.8.1.78)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.11.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.23.5)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from easyocr) (9.4.0)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.19.3)\n","Collecting python-bidi (from easyocr)\n","  Downloading python_bidi-0.4.2-py2.py3-none-any.whl (30 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from easyocr) (6.0.1)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.0.2)\n","Collecting pyclipper (from easyocr)\n","  Downloading pyclipper-1.3.0.post5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (908 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ninja (from easyocr)\n","  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5->easyocr) (2.31.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (2.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from python-bidi->easyocr) (1.16.0)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2.31.6)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2023.9.26)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (1.4.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (23.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->easyocr) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5->easyocr) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5->easyocr) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5->easyocr) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5->easyocr) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->easyocr) (1.3.0)\n","Installing collected packages: pyclipper, ninja, python-bidi, easyocr\n","Successfully installed easyocr-1.7.1 ninja-1.11.1.1 pyclipper-1.3.0.post5 python-bidi-0.4.2\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/MomentCapture/Colab/EasyOCR/trainer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qFRK3ql44DdN","executionInfo":{"status":"ok","timestamp":1700039684925,"user_tz":-540,"elapsed":8,"user":{"displayName":"michelle","userId":"15485239787053820654"}},"outputId":"27c07446-9ddf-405e-fd98-4c6a7da4d3f9"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/MomentCapture/Colab/EasyOCR/trainer\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"ExecuteTime":{"end_time":"2021-07-23T04:19:23.488642Z","start_time":"2021-07-23T04:19:21.854534Z"},"id":"Vwnx5LZDwFIJ","executionInfo":{"status":"ok","timestamp":1700039691132,"user_tz":-540,"elapsed":6212,"user":{"displayName":"michelle","userId":"15485239787053820654"}}},"outputs":[],"source":["import os\n","import torch.backends.cudnn as cudnn\n","import yaml\n","from utils import AttrDict\n","import pandas as pd"]},{"cell_type":"markdown","source":["## **test**"],"metadata":{"id":"odE8IqXqFpCU"}},{"cell_type":"code","source":["import os\n","import time\n","import string\n","import argparse\n","\n","import torch\n","import torch.backends.cudnn as cudnn\n","import torch.utils.data\n","import torch.nn.functional as F\n","import numpy as np\n","from nltk.metrics.distance import edit_distance\n","\n","from utils import CTCLabelConverter, AttnLabelConverter, Averager\n","from dataset import hierarchical_dataset, AlignCollate\n","from model import Model\n","\n","def validation(model, criterion, evaluation_loader, converter, opt, device):\n","    \"\"\" validation or evaluation \"\"\"\n","    n_correct = 0\n","    norm_ED = 0\n","    length_of_data = 0\n","    infer_time = 0\n","    valid_loss_avg = Averager()\n","\n","    for i, (image_tensors, labels) in enumerate(evaluation_loader):\n","        batch_size = image_tensors.size(0)\n","        length_of_data = length_of_data + batch_size\n","        image = image_tensors.to(device)\n","        # For max length prediction\n","        length_for_pred = torch.IntTensor([opt.batch_max_length] * batch_size).to(device)\n","        text_for_pred = torch.LongTensor(batch_size, opt.batch_max_length + 1).fill_(0).to(device)\n","\n","        text_for_loss, length_for_loss = converter.encode(labels, batch_max_length=opt.batch_max_length)\n","\n","        start_time = time.time()\n","        if 'CTC' in opt.Prediction:\n","            preds = model(image, text_for_pred)\n","            forward_time = time.time() - start_time\n","\n","            # Calculate evaluation loss for CTC decoder.\n","            preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n","            # permute 'preds' to use CTCloss format\n","            cost = criterion(preds.log_softmax(2).permute(1, 0, 2), text_for_loss, preds_size, length_for_loss)\n","\n","            if opt.decode == 'greedy':\n","                # Select max probabilty (greedy decoding) then decode index to character\n","                _, preds_index = preds.max(2)\n","                preds_index = preds_index.view(-1)\n","                preds_str = converter.decode_greedy(preds_index.data, preds_size.data)\n","            elif opt.decode == 'beamsearch':\n","                preds_str = converter.decode_beamsearch(preds, beamWidth=2)\n","\n","        else:\n","            preds = model(image, text_for_pred, is_train=False)\n","            forward_time = time.time() - start_time\n","\n","            preds = preds[:, :text_for_loss.shape[1] - 1, :]\n","            target = text_for_loss[:, 1:]  # without [GO] Symbol\n","            cost = criterion(preds.contiguous().view(-1, preds.shape[-1]), target.contiguous().view(-1))\n","\n","            # select max probabilty (greedy decoding) then decode index to character\n","            _, preds_index = preds.max(2)\n","            preds_str = converter.decode(preds_index, length_for_pred)\n","            labels = converter.decode(text_for_loss[:, 1:], length_for_loss)\n","\n","        infer_time += forward_time\n","        valid_loss_avg.add(cost)\n","\n","        # calculate accuracy & confidence score\n","        preds_prob = F.softmax(preds, dim=2)\n","        preds_max_prob, _ = preds_prob.max(dim=2)\n","        confidence_score_list = []\n","\n","        for gt, pred, pred_max_prob in zip(labels, preds_str, preds_max_prob):\n","            if 'Attn' in opt.Prediction:\n","                gt = gt[:gt.find('[s]')]\n","                pred_EOS = pred.find('[s]')\n","                pred = pred[:pred_EOS]  # prune after \"end of sentence\" token ([s])\n","                pred_max_prob = pred_max_prob[:pred_EOS]\n","\n","            if pred == gt:\n","                n_correct += 1\n","\n","            '''\n","            (old version) ICDAR2017 DOST Normalized Edit Distance https://rrc.cvc.uab.es/?ch=7&com=tasks\n","            \"For each word we calculate the normalized edit distance to the length of the ground truth transcription.\"\n","            if len(gt) == 0:\n","                norm_ED += 1\n","            else:\n","                norm_ED += edit_distance(pred, gt) / len(gt)\n","            '''\n","\n","            # ICDAR2019 Normalized Edit Distance\n","            if len(gt) == 0 or len(pred) ==0:\n","                norm_ED += 0\n","            elif len(gt) > len(pred):\n","                norm_ED += 1 - edit_distance(pred, gt) / len(gt)\n","            else:\n","                norm_ED += 1 - edit_distance(pred, gt) / len(pred)\n","\n","            # calculate confidence score (= multiply of pred_max_prob)\n","            try:\n","                confidence_score = pred_max_prob.cumprod(dim=0)[-1]\n","            except:\n","                confidence_score = 0  # for empty pred case, when prune after \"end of sentence\" token ([s])\n","            confidence_score_list.append(confidence_score)\n","            # print(pred, gt, pred==gt, confidence_score)\n","\n","    accuracy = n_correct / float(length_of_data) * 100\n","    norm_ED = norm_ED / float(length_of_data) # ICDAR2019 Normalized Edit Distance\n","\n","    return valid_loss_avg.val(), accuracy, norm_ED, preds_str, confidence_score_list, labels, infer_time, length_of_data"],"metadata":{"id":"cyY8EfJC6ZJc","executionInfo":{"status":"ok","timestamp":1700039696278,"user_tz":-540,"elapsed":5149,"user":{"displayName":"michelle","userId":"15485239787053820654"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## **train**"],"metadata":{"id":"LLoOljdoFkUo"}},{"cell_type":"code","source":["import os\n","import sys\n","import time\n","import random\n","import torch\n","import torch.backends.cudnn as cudnn\n","import torch.nn as nn\n","import torch.nn.init as init\n","import torch.optim as optim\n","import torch.utils.data\n","from torch.cuda.amp import autocast, GradScaler\n","import numpy as np\n","\n","from utils import CTCLabelConverter, AttnLabelConverter, Averager\n","from dataset import hierarchical_dataset, AlignCollate, Batch_Balanced_Dataset\n","from model import Model\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","def count_parameters(model):\n","    print(\"Modules, Parameters\")\n","    total_params = 0\n","    for name, parameter in model.named_parameters():\n","        if not parameter.requires_grad: continue\n","        param = parameter.numel()\n","        #table.add_row([name, param])\n","        total_params+=param\n","        print(name, param)\n","    print(f\"Total Trainable Params: {total_params}\")\n","    return total_params\n","\n","def train(opt, show_number = 2, amp=False):\n","    \"\"\" dataset preparation \"\"\"\n","    if not opt.data_filtering_off:\n","        print('Filtering the images containing characters which are not in opt.character')\n","        print('Filtering the images whose label is longer than opt.batch_max_length')\n","\n","    opt.select_data = opt.select_data.split('-')\n","    opt.batch_ratio = opt.batch_ratio.split('-')\n","    train_dataset = Batch_Balanced_Dataset(opt)\n","\n","    log = open(f'./saved_models/{opt.experiment_name}/log_dataset.txt', 'a', encoding=\"utf8\")\n","    AlignCollate_valid = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD, contrast_adjust=opt.contrast_adjust)\n","    valid_dataset, valid_dataset_log = hierarchical_dataset(root=opt.valid_data, opt=opt)\n","    valid_loader = torch.utils.data.DataLoader(\n","        valid_dataset, batch_size=min(32, opt.batch_size),\n","        shuffle=True,  # 'True' to check training progress with validation function.\n","        num_workers=int(opt.workers), prefetch_factor=512,\n","        collate_fn=AlignCollate_valid, pin_memory=True)\n","    log.write(valid_dataset_log)\n","    print('-' * 80)\n","    log.write('-' * 80 + '\\n')\n","    log.close()\n","\n","    \"\"\" model configuration \"\"\"\n","    if 'CTC' in opt.Prediction:\n","        converter = CTCLabelConverter(opt.character)\n","    else:\n","        converter = AttnLabelConverter(opt.character)\n","    opt.num_class = len(converter.character)\n","\n","    if opt.rgb:\n","        opt.input_channel = 3\n","    model = Model(opt)\n","    print('model input parameters', opt.imgH, opt.imgW, opt.num_fiducial, opt.input_channel, opt.output_channel,\n","          opt.hidden_size, opt.num_class, opt.batch_max_length, opt.Transformation, opt.FeatureExtraction,\n","          opt.SequenceModeling, opt.Prediction)\n","\n","    if opt.saved_model != '':\n","        pretrained_dict = torch.load(opt.saved_model)\n","        if opt.new_prediction:\n","            model.Prediction = nn.Linear(model.SequenceModeling_output, len(pretrained_dict['module.Prediction.weight']))\n","\n","        model = torch.nn.DataParallel(model).to(device)\n","        print(f'loading pretrained model from {opt.saved_model}')\n","        if opt.FT:\n","            model.load_state_dict(pretrained_dict, strict=False)\n","        else:\n","            model.load_state_dict(pretrained_dict)\n","        if opt.new_prediction:\n","            model.module.Prediction = nn.Linear(model.module.SequenceModeling_output, opt.num_class)\n","            for name, param in model.module.Prediction.named_parameters():\n","                if 'bias' in name:\n","                    init.constant_(param, 0.0)\n","                elif 'weight' in name:\n","                    init.kaiming_normal_(param)\n","            model = model.to(device)\n","    else:\n","        # weight initialization\n","        for name, param in model.named_parameters():\n","            if 'localization_fc2' in name:\n","                print(f'Skip {name} as it is already initialized')\n","                continue\n","            try:\n","                if 'bias' in name:\n","                    init.constant_(param, 0.0)\n","                elif 'weight' in name:\n","                    init.kaiming_normal_(param)\n","            except Exception as e:  # for batchnorm.\n","                if 'weight' in name:\n","                    param.data.fill_(1)\n","                continue\n","        model = torch.nn.DataParallel(model).to(device)\n","\n","    model.train()\n","    print(\"Model:\")\n","    print(model)\n","    count_parameters(model)\n","\n","    \"\"\" setup loss \"\"\"\n","    if 'CTC' in opt.Prediction:\n","        criterion = torch.nn.CTCLoss(zero_infinity=True).to(device)\n","    else:\n","        criterion = torch.nn.CrossEntropyLoss(ignore_index=0).to(device)  # ignore [GO] token = ignore index 0\n","    # loss averager\n","    loss_avg = Averager()\n","\n","    # freeze some layers\n","    try:\n","        if opt.freeze_FeatureFxtraction:\n","            for param in model.module.FeatureExtraction.parameters():\n","                param.requires_grad = False\n","        if opt.freeze_SequenceModeling:\n","            for param in model.module.SequenceModeling.parameters():\n","                param.requires_grad = False\n","    except:\n","        pass\n","\n","    # filter that only require gradient decent\n","    filtered_parameters = []\n","    params_num = []\n","    for p in filter(lambda p: p.requires_grad, model.parameters()):\n","        filtered_parameters.append(p)\n","        params_num.append(np.prod(p.size()))\n","    print('Trainable params num : ', sum(params_num))\n","    # [print(name, p.numel()) for name, p in filter(lambda p: p[1].requires_grad, model.named_parameters())]\n","\n","    # setup optimizer\n","    if opt.optim=='adam':\n","        #optimizer = optim.Adam(filtered_parameters, lr=opt.lr, betas=(opt.beta1, 0.999))\n","        optimizer = optim.Adam(filtered_parameters)\n","    else:\n","        optimizer = optim.Adadelta(filtered_parameters, lr=opt.lr, rho=opt.rho, eps=opt.eps)\n","    print(\"Optimizer:\")\n","    print(optimizer)\n","\n","    \"\"\" final options \"\"\"\n","    # print(opt)\n","    with open(f'./saved_models/{opt.experiment_name}/opt.txt', 'a', encoding=\"utf8\") as opt_file:\n","        opt_log = '------------ Options -------------\\n'\n","        args = vars(opt)\n","        for k, v in args.items():\n","            opt_log += f'{str(k)}: {str(v)}\\n'\n","        opt_log += '---------------------------------------\\n'\n","        print(opt_log)\n","        opt_file.write(opt_log)\n","\n","    \"\"\" start training \"\"\"\n","    start_iter = 0\n","    if opt.saved_model != '':\n","        try:\n","            start_iter = int(opt.saved_model.split('_')[-1].split('.')[0])\n","            print(f'continue to train, start_iter: {start_iter}')\n","        except:\n","            pass\n","\n","    start_time = time.time()\n","    best_accuracy = -1\n","    best_norm_ED = -1\n","    i = start_iter\n","\n","    scaler = GradScaler()\n","    t1= time.time()\n","\n","    while(True):\n","        # train part\n","        optimizer.zero_grad(set_to_none=True)\n","\n","        if amp:\n","            with autocast():\n","                image_tensors, labels = train_dataset.get_batch()\n","                image = image_tensors.to(device)\n","                text, length = converter.encode(labels, batch_max_length=opt.batch_max_length)\n","                batch_size = image.size(0)\n","\n","                if 'CTC' in opt.Prediction:\n","                    preds = model(image, text).log_softmax(2)\n","                    preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n","                    preds = preds.permute(1, 0, 2)\n","                    torch.backends.cudnn.enabled = False\n","                    cost = criterion(preds, text.to(device), preds_size.to(device), length.to(device))\n","                    torch.backends.cudnn.enabled = True\n","                else:\n","                    preds = model(image, text[:, :-1])  # align with Attention.forward\n","                    target = text[:, 1:]  # without [GO] Symbol\n","                    cost = criterion(preds.view(-1, preds.shape[-1]), target.contiguous().view(-1))\n","            scaler.scale(cost).backward()\n","            scaler.unscale_(optimizer)\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), opt.grad_clip)\n","            scaler.step(optimizer)\n","            scaler.update()\n","        else:\n","            image_tensors, labels = train_dataset.get_batch()\n","            image = image_tensors.to(device)\n","            text, length = converter.encode(labels, batch_max_length=opt.batch_max_length)\n","            batch_size = image.size(0)\n","            if 'CTC' in opt.Prediction:\n","                preds = model(image, text).log_softmax(2)\n","                preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n","                preds = preds.permute(1, 0, 2)\n","                torch.backends.cudnn.enabled = False\n","                cost = criterion(preds, text.to(device), preds_size.to(device), length.to(device))\n","                torch.backends.cudnn.enabled = True\n","            else:\n","                preds = model(image, text[:, :-1])  # align with Attention.forward\n","                target = text[:, 1:]  # without [GO] Symbol\n","                cost = criterion(preds.view(-1, preds.shape[-1]), target.contiguous().view(-1))\n","            cost.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), opt.grad_clip)\n","            optimizer.step()\n","        loss_avg.add(cost)\n","\n","        # validation part\n","        if (i % opt.valInterval == 0) and (i!=0):\n","            print('training time: ', time.time()-t1)\n","            t1=time.time()\n","            elapsed_time = time.time() - start_time\n","            # for log\n","            with open(f'./saved_models/{opt.experiment_name}/log_train.txt', 'a', encoding=\"utf8\") as log:\n","                model.eval()\n","                with torch.no_grad():\n","                    valid_loss, current_accuracy, current_norm_ED, preds, confidence_score, labels,\\\n","                    infer_time, length_of_data = validation(model, criterion, valid_loader, converter, opt, device)\n","                model.train()\n","\n","                # training loss and validation loss\n","                loss_log = f'[{i}/{opt.num_iter}] Train loss: {loss_avg.val():0.5f}, Valid loss: {valid_loss:0.5f}, Elapsed_time: {elapsed_time:0.5f}'\n","                loss_avg.reset()\n","\n","                current_model_log = f'{\"Current_accuracy\":17s}: {current_accuracy:0.3f}, {\"Current_norm_ED\":17s}: {current_norm_ED:0.4f}'\n","\n","                # keep best accuracy model (on valid dataset)\n","                if current_accuracy > best_accuracy:\n","                    best_accuracy = current_accuracy\n","                    torch.save(model.state_dict(), f'./saved_models/{opt.experiment_name}/best_accuracy.pth')\n","                if current_norm_ED > best_norm_ED:\n","                    best_norm_ED = current_norm_ED\n","                    torch.save(model.state_dict(), f'./saved_models/{opt.experiment_name}/best_norm_ED.pth')\n","                best_model_log = f'{\"Best_accuracy\":17s}: {best_accuracy:0.3f}, {\"Best_norm_ED\":17s}: {best_norm_ED:0.4f}'\n","\n","                loss_model_log = f'{loss_log}\\n{current_model_log}\\n{best_model_log}'\n","                print(loss_model_log)\n","                log.write(loss_model_log + '\\n')\n","\n","                # show some predicted results\n","                dashed_line = '-' * 80\n","                head = f'{\"Ground Truth\":25s} | {\"Prediction\":25s} | Confidence Score & T/F'\n","                predicted_result_log = f'{dashed_line}\\n{head}\\n{dashed_line}\\n'\n","\n","                #show_number = min(show_number, len(labels))\n","\n","                start = random.randint(0,len(labels) - show_number )\n","                for gt, pred, confidence in zip(labels[start:start+show_number], preds[start:start+show_number], confidence_score[start:start+show_number]):\n","                    if 'Attn' in opt.Prediction:\n","                        gt = gt[:gt.find('[s]')]\n","                        pred = pred[:pred.find('[s]')]\n","\n","                    predicted_result_log += f'{gt:25s} | {pred:25s} | {confidence:0.4f}\\t{str(pred == gt)}\\n'\n","                predicted_result_log += f'{dashed_line}'\n","                print(predicted_result_log)\n","                log.write(predicted_result_log + '\\n')\n","                print('validation time: ', time.time()-t1)\n","                t1=time.time()\n","        # save model per 1e+4 iter.\n","        if (i + 1) % 1e+4 == 0:\n","            torch.save(\n","                model.state_dict(), f'./saved_models/{opt.experiment_name}/iter_{i+1}.pth')\n","\n","        if i == opt.num_iter:\n","            print('end the training')\n","            sys.exit()\n","        i += 1"],"metadata":{"id":"InX3UKem5FAd","executionInfo":{"status":"ok","timestamp":1700039696279,"user_tz":-540,"elapsed":8,"user":{"displayName":"michelle","userId":"15485239787053820654"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## **retrain**"],"metadata":{"id":"szDNHs9jOO_5"}},{"cell_type":"code","execution_count":7,"metadata":{"ExecuteTime":{"end_time":"2021-07-23T04:19:23.885144Z","start_time":"2021-07-23T04:19:23.880564Z"},"code_folding":[],"id":"M1oxRy2RwFIY","executionInfo":{"status":"ok","timestamp":1700039696279,"user_tz":-540,"elapsed":8,"user":{"displayName":"michelle","userId":"15485239787053820654"}}},"outputs":[],"source":["cudnn.benchmark = True\n","cudnn.deterministic = False"]},{"cell_type":"code","execution_count":8,"metadata":{"ExecuteTime":{"end_time":"2021-07-23T04:19:24.119144Z","start_time":"2021-07-23T04:19:24.112032Z"},"code_folding":[0],"id":"gsZTIuImwFIa","executionInfo":{"status":"ok","timestamp":1700039696279,"user_tz":-540,"elapsed":7,"user":{"displayName":"michelle","userId":"15485239787053820654"}}},"outputs":[],"source":["def get_config(file_path):\n","    with open(file_path, 'r', encoding=\"utf8\") as stream:\n","        opt = yaml.safe_load(stream)\n","    opt = AttrDict(opt)\n","    if opt.lang_char == 'None':\n","        characters = ''\n","        for data in opt['select_data'].split('-'):\n","            csv_path = os.path.join(opt['train_data'], data, 'labels.csv')\n","            df = pd.read_csv(csv_path, sep='^([^,]+),', engine='python', usecols=['filename', 'words'], keep_default_na=False)\n","            all_char = ''.join(df['words'])\n","            characters += ''.join(set(all_char))\n","        characters = sorted(set(characters))\n","        opt.character= ''.join(characters)\n","    else:\n","        opt.character = opt.number + opt.symbol + opt.lang_char\n","    os.makedirs(f'./saved_models/{opt.experiment_name}', exist_ok=True)\n","    return opt"]},{"cell_type":"code","execution_count":11,"metadata":{"ExecuteTime":{"end_time":"2021-07-23T04:49:07.045060Z","start_time":"2021-07-23T04:20:15.050992Z"},"id":"feXvl-d6wFId","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1700040078603,"user_tz":-540,"elapsed":1554,"user":{"displayName":"michelle","userId":"15485239787053820654"}},"outputId":"92c8ceb2-bd9e-4cbe-ca5a-fc237529056b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Filtering the images containing characters which are not in opt.character\n","Filtering the images whose label is longer than opt.batch_max_length\n","--------------------------------------------------------------------------------\n","dataset_root: all_data\n","opt.select_data: ['en_train_filtered']\n","opt.batch_ratio: ['1']\n","--------------------------------------------------------------------------------\n","dataset_root:    all_data\t dataset: en_train_filtered\n","all_data/en_train_filtered\n","19\n","21\n","14\n","28\n","24\n","13\n","20\n","12\n","16\n","27\n","17\n","5\n","9\n","10\n","26\n","11\n","7\n","15\n","3\n","23\n","25\n","4\n","6\n","8\n","22\n","18\n","2\n","18\n","17\n","1\n","16\n","20\n","19\n","21\n","17\n","16\n","18\n","14\n","20\n","22\n","23\n","13\n","11\n","3\n","15\n","12\n","9\n","5\n","8\n","14\n","13\n","28\n","6\n","30\n","29\n","11\n","1\n","24\n","12\n","23\n","26\n","10\n","15\n","19\n","25\n","27\n","4\n","7\n","2\n","16\n","18\n","22\n","21\n","17\n","7\n","13\n","15\n","14\n","20\n","7\n","9\n","6\n","10\n","12\n","6\n","10\n","4\n","20\n","8\n","1\n","9\n","2\n","3\n","8\n","11\n","5\n","15\n","18\n","19\n","17\n","14\n","10\n","8\n","12\n","3\n","9\n","18\n","2\n","5\n","1\n","19\n","20\n","16\n","7\n","6\n","11\n","12\n","11\n","15\n","13\n","16\n","2\n","4\n","13\n","5\n","1\n","17\n","3\n","14\n","7\n","10\n","6\n","19\n","3\n","1\n","9\n","20\n","5\n","4\n","17\n","19\n","12\n","8\n","14\n","9\n","2\n","4\n","15\n","8\n","6\n","10\n","11\n","16\n","13\n","3\n","18\n","1\n","44\n","7\n","42\n","45\n","40\n","38\n","2\n","39\n","5\n","35\n","33\n","34\n","45\n","41\n","32\n","37\n","30\n","31\n","19\n","20\n","18\n","44\n","36\n","4\n","29\n","17\n","43\n","sub-directory:\t/en_train_filtered\t num samples: 184\n","num total samples of en_train_filtered: 184 x 1.0 (total_data_usage_ratio) = 184\n","num samples of en_train_filtered per batch: 32 x 1.0 (batch_ratio) = 32\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","Total_batch_size: 32 = 32\n","--------------------------------------------------------------------------------\n","dataset_root:    all_data/en_val\t dataset: /\n","all_data/en_val/\n","1\n","4\n","2\n","3\n","16\n","14\n","18\n","19\n","7\n","17\n","8\n","13\n","3\n","20\n","20\n","1\n","9\n","2\n","6\n","10\n","18\n","16\n","19\n","15\n","5\n","17\n","4\n","12\n","14\n","11\n","8\n","12\n","10\n","5\n","9\n","2\n","6\n","11\n","29\n","7\n","15\n","3\n","30\n","1\n","25\n","13\n","28\n","4\n","27\n","24\n","26\n","sub-directory:\t/.\t num samples: 51\n","--------------------------------------------------------------------------------\n","No Transformation module specified\n","model input parameters 64 600 20 1 256 256 97 34 None VGG BiLSTM CTC\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Model:\n","DataParallel(\n","  (module): Model(\n","    (FeatureExtraction): VGG_FeatureExtractor(\n","      (ConvNet): Sequential(\n","        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","        (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (4): ReLU(inplace=True)\n","        (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","        (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (7): ReLU(inplace=True)\n","        (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (9): ReLU(inplace=True)\n","        (10): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n","        (11): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (13): ReLU(inplace=True)\n","        (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (16): ReLU(inplace=True)\n","        (17): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n","        (18): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1))\n","        (19): ReLU(inplace=True)\n","      )\n","    )\n","    (AdaptiveAvgPool): AdaptiveAvgPool2d(output_size=(None, 1))\n","    (SequenceModeling): Sequential(\n","      (0): BidirectionalLSTM(\n","        (rnn): LSTM(256, 256, batch_first=True, bidirectional=True)\n","        (linear): Linear(in_features=512, out_features=256, bias=True)\n","      )\n","      (1): BidirectionalLSTM(\n","        (rnn): LSTM(256, 256, batch_first=True, bidirectional=True)\n","        (linear): Linear(in_features=512, out_features=256, bias=True)\n","      )\n","    )\n","    (Prediction): Linear(in_features=256, out_features=97, bias=True)\n","  )\n",")\n","Modules, Parameters\n","module.FeatureExtraction.ConvNet.0.weight 288\n","module.FeatureExtraction.ConvNet.0.bias 32\n","module.FeatureExtraction.ConvNet.3.weight 18432\n","module.FeatureExtraction.ConvNet.3.bias 64\n","module.FeatureExtraction.ConvNet.6.weight 73728\n","module.FeatureExtraction.ConvNet.6.bias 128\n","module.FeatureExtraction.ConvNet.8.weight 147456\n","module.FeatureExtraction.ConvNet.8.bias 128\n","module.FeatureExtraction.ConvNet.11.weight 294912\n","module.FeatureExtraction.ConvNet.12.weight 256\n","module.FeatureExtraction.ConvNet.12.bias 256\n","module.FeatureExtraction.ConvNet.14.weight 589824\n","module.FeatureExtraction.ConvNet.15.weight 256\n","module.FeatureExtraction.ConvNet.15.bias 256\n","module.FeatureExtraction.ConvNet.18.weight 262144\n","module.FeatureExtraction.ConvNet.18.bias 256\n","module.SequenceModeling.0.rnn.weight_ih_l0 262144\n","module.SequenceModeling.0.rnn.weight_hh_l0 262144\n","module.SequenceModeling.0.rnn.bias_ih_l0 1024\n","module.SequenceModeling.0.rnn.bias_hh_l0 1024\n","module.SequenceModeling.0.rnn.weight_ih_l0_reverse 262144\n","module.SequenceModeling.0.rnn.weight_hh_l0_reverse 262144\n","module.SequenceModeling.0.rnn.bias_ih_l0_reverse 1024\n","module.SequenceModeling.0.rnn.bias_hh_l0_reverse 1024\n","module.SequenceModeling.0.linear.weight 131072\n","module.SequenceModeling.0.linear.bias 256\n","module.SequenceModeling.1.rnn.weight_ih_l0 262144\n","module.SequenceModeling.1.rnn.weight_hh_l0 262144\n","module.SequenceModeling.1.rnn.bias_ih_l0 1024\n","module.SequenceModeling.1.rnn.bias_hh_l0 1024\n","module.SequenceModeling.1.rnn.weight_ih_l0_reverse 262144\n","module.SequenceModeling.1.rnn.weight_hh_l0_reverse 262144\n","module.SequenceModeling.1.rnn.bias_ih_l0_reverse 1024\n","module.SequenceModeling.1.rnn.bias_hh_l0_reverse 1024\n","module.SequenceModeling.1.linear.weight 131072\n","module.SequenceModeling.1.linear.bias 256\n","module.Prediction.weight 24832\n","module.Prediction.bias 97\n","Total Trainable Params: 3781345\n","Trainable params num :  3781345\n","Optimizer:\n","Adadelta (\n","Parameter Group 0\n","    differentiable: False\n","    eps: 1e-08\n","    foreach: None\n","    lr: 1.0\n","    maximize: False\n","    rho: 0.95\n","    weight_decay: 0\n",")\n","------------ Options -------------\n","number: 0123456789\n","symbol: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ €\n","lang_char: ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n","experiment_name: en_filtered\n","train_data: all_data\n","valid_data: all_data/en_val\n","manualSeed: 1111\n","workers: 6\n","batch_size: 32\n","num_iter: 300000\n","valInterval: 20000\n","saved_model: \n","FT: False\n","optim: False\n","lr: 1.0\n","beta1: 0.9\n","rho: 0.95\n","eps: 1e-08\n","grad_clip: 5\n","select_data: ['en_train_filtered']\n","batch_ratio: ['1']\n","total_data_usage_ratio: 1.0\n","batch_max_length: 34\n","imgH: 64\n","imgW: 600\n","rgb: False\n","contrast_adjust: 0.0\n","sensitive: True\n","PAD: True\n","data_filtering_off: False\n","Transformation: None\n","FeatureExtraction: VGG\n","SequenceModeling: BiLSTM\n","Prediction: CTC\n","num_fiducial: 20\n","input_channel: 1\n","output_channel: 256\n","hidden_size: 256\n","decode: greedy\n","new_prediction: False\n","freeze_FeatureFxtraction: False\n","freeze_SequenceModeling: False\n","character: 0123456789!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ €ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n","num_class: 97\n","---------------------------------------\n","\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-6d4c06a1ca45>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"config_files/en_filtered_config.yaml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-6-f75e44f5a9ee>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(opt, show_number, amp)\u001b[0m\n\u001b[1;32m    210\u001b[0m                 \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                 \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, log_probs, targets, input_lengths, target_lengths)\u001b[0m\n\u001b[1;32m   1768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1769\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1770\u001b[0;31m         return F.ctc_loss(log_probs, targets, input_lengths, target_lengths, self.blank, self.reduction,\n\u001b[0m\u001b[1;32m   1771\u001b[0m                           self.zero_infinity)\n\u001b[1;32m   1772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mctc_loss\u001b[0;34m(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity)\u001b[0m\n\u001b[1;32m   2654\u001b[0m             \u001b[0mblank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_infinity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzero_infinity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2655\u001b[0m         )\n\u001b[0;32m-> 2656\u001b[0;31m     return torch.ctc_loss(\n\u001b[0m\u001b[1;32m   2657\u001b[0m         \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_infinity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m     )\n","\u001b[0;31mRuntimeError\u001b[0m: target_lengths must be of size batch_size"]}],"source":["opt = get_config(\"config_files/en_filtered_config.yaml\")\n","train(opt, amp=False)"]},{"cell_type":"markdown","source":["# **end**"],"metadata":{"id":"LixKxLW2AJSF"}},{"cell_type":"code","source":[],"metadata":{"id":"STP4g6NgAOAu"},"execution_count":null,"outputs":[]}]}